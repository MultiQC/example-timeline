{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulating MultiQC runs with exporting of data into two types of intermediate format:\n",
    "- columnar data lake (parquet)\n",
    "- document DB (MongoDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import shutil\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pymongo\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "# Configuration parameters\n",
    "NUM_RUNS = 1000  # Can be scaled up to millions in real case\n",
    "NUM_MODULES = 10  # Fixed across runs\n",
    "NUM_SAMPLES_PER_MODULE = 100  # Can be 10 to 1000\n",
    "NUM_METRICS_PER_MODULE = 20  # Can be 10 to 50\n",
    "\n",
    "# MongoDB setup\n",
    "MONGO_DB_NAME = \"data_comparison\"\n",
    "MONGO_COLLECTION_NAME = \"runs\"\n",
    "MONGO_URI = \"mongodb://localhost:27017/\"\n",
    "\n",
    "# Parquet setup\n",
    "PARQUET_DIR = \"parquet_data\"\n",
    "os.makedirs(PARQUET_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def generate_random_string(length=10):\n",
    "    \"\"\"Generate a random string of fixed length\"\"\"\n",
    "    return \"\".join(random.choices(string.ascii_letters, k=length))\n",
    "\n",
    "\n",
    "def generate_metric_metadata():\n",
    "    \"\"\"Generate metadata for a metric\"\"\"\n",
    "    return {\n",
    "        \"min\": random.uniform(0, 10),\n",
    "        \"max\": random.uniform(90, 100),\n",
    "        \"scale\": random.choice([\"linear\", \"log\"]),\n",
    "        \"color\": f\"#{random.randint(0, 0xFFFFFF):06x}\",\n",
    "        \"type\": random.choice([\"numeric\", \"categorical\", \"percentage\"]),\n",
    "        \"namespace\": random.choice([\"performance\", \"quality\", \"resource\"]),\n",
    "        \"placement\": random.choice([\"primary\", \"secondary\", \"tertiary\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_value_metadata(value):\n",
    "    \"\"\"Generate metadata for a value\"\"\"\n",
    "    return {\n",
    "        \"unmodified_value\": value,\n",
    "        \"formatted_value\": f\"{value:.2f}\" if isinstance(value, float) else str(value),\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_sample_data(num_metrics):\n",
    "    \"\"\"Generate data for a single sample\"\"\"\n",
    "    sample_id = generate_random_string()\n",
    "    metrics = {}\n",
    "\n",
    "    for i in range(num_metrics):\n",
    "        metric_name = f\"metric_{i}\"\n",
    "        value = random.uniform(0, 100)\n",
    "        metrics[metric_name] = {\n",
    "            \"value\": value,\n",
    "            \"metadata\": generate_value_metadata(value),\n",
    "        }\n",
    "\n",
    "    return {\"sample_id\": sample_id, \"metrics\": metrics}\n",
    "\n",
    "\n",
    "def generate_module_data(module_index, num_samples, num_metrics):\n",
    "    \"\"\"Generate data for a single module\"\"\"\n",
    "    samples = [generate_sample_data(num_metrics) for _ in range(num_samples)]\n",
    "\n",
    "    metrics_metadata = {}\n",
    "    for i in range(num_metrics):\n",
    "        metric_name = f\"metric_{i}\"\n",
    "        metrics_metadata[metric_name] = generate_metric_metadata()\n",
    "\n",
    "    return {\n",
    "        \"module_id\": f\"module_{module_index}\",\n",
    "        \"name\": f\"Module {module_index}\",\n",
    "        \"url\": f\"http://example.com/module/{module_index}\",\n",
    "        \"comment\": f\"This is module {module_index}\",\n",
    "        \"metrics_metadata\": metrics_metadata,\n",
    "        \"samples\": samples,\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_run_data(\n",
    "    run_index, num_modules, num_samples_per_module, num_metrics_per_module\n",
    "):\n",
    "    \"\"\"Generate data for a single run\"\"\"\n",
    "    modules = [\n",
    "        generate_module_data(i, num_samples_per_module, num_metrics_per_module)\n",
    "        for i in range(num_modules)\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"run_id\": f\"run_{run_index}\",\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"modules\": modules,\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_all_data(\n",
    "    num_runs, num_modules, num_samples_per_module, num_metrics_per_module\n",
    "):\n",
    "    \"\"\"Generate all runs data\"\"\"\n",
    "    return [\n",
    "        generate_run_data(\n",
    "            i, num_modules, num_samples_per_module, num_metrics_per_module\n",
    "        )\n",
    "        for i in range(num_runs)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_in_mongodb(data, uri=MONGO_URI, db_name=MONGO_DB_NAME, collection_name=MONGO_COLLECTION_NAME):\n",
    "    \"\"\"Store the hierarchical data in MongoDB\"\"\"\n",
    "    client = pymongo.MongoClient(uri)\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    # Drop the collection if it already exists\n",
    "    collection.drop()\n",
    "\n",
    "    # Insert the data\n",
    "    start_time = time.time()\n",
    "    collection.insert_many(data)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Create indexes for common queries\n",
    "    collection.create_index([(\"run_id\", pymongo.ASCENDING)])\n",
    "    collection.create_index([(\"modules.module_id\", pymongo.ASCENDING)])\n",
    "    collection.create_index([(\"modules.samples.sample_id\", pymongo.ASCENDING)])\n",
    "\n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "def flatten_hierarchical_data(data):\n",
    "    \"\"\"Convert hierarchical data to flat format for Parquet\"\"\"\n",
    "    flat_records = []\n",
    "\n",
    "    for run in data:\n",
    "        run_id = run[\"run_id\"]\n",
    "        timestamp = run[\"timestamp\"]\n",
    "\n",
    "        for module in run[\"modules\"]:\n",
    "            module_id = module[\"module_id\"]\n",
    "            module_name = module[\"name\"]\n",
    "            module_url = module[\"url\"]\n",
    "            module_comment = module[\"comment\"]\n",
    "\n",
    "            for sample in module[\"samples\"]:\n",
    "                sample_id = sample[\"sample_id\"]\n",
    "\n",
    "                for metric_name, metric_data in sample[\"metrics\"].items():\n",
    "                    value = metric_data[\"value\"]\n",
    "                    unmodified_value = metric_data[\"metadata\"][\"unmodified_value\"]\n",
    "                    formatted_value = metric_data[\"metadata\"][\"formatted_value\"]\n",
    "\n",
    "                    # Get metric metadata\n",
    "                    metric_metadata = module[\"metrics_metadata\"].get(metric_name, {})\n",
    "\n",
    "                    flat_records.append(\n",
    "                        {\n",
    "                            \"run_id\": run_id,\n",
    "                            \"timestamp\": timestamp,\n",
    "                            \"module_id\": module_id,\n",
    "                            \"module_name\": module_name,\n",
    "                            \"module_url\": module_url,\n",
    "                            \"module_comment\": module_comment,\n",
    "                            \"sample_id\": sample_id,\n",
    "                            \"metric_name\": metric_name,\n",
    "                            \"value\": value,\n",
    "                            \"unmodified_value\": unmodified_value,\n",
    "                            \"formatted_value\": formatted_value,\n",
    "                            \"metric_min\": metric_metadata.get(\"min\"),\n",
    "                            \"metric_max\": metric_metadata.get(\"max\"),\n",
    "                            \"metric_scale\": metric_metadata.get(\"scale\"),\n",
    "                            \"metric_color\": metric_metadata.get(\"color\"),\n",
    "                            \"metric_type\": metric_metadata.get(\"type\"),\n",
    "                            \"metric_namespace\": metric_metadata.get(\"namespace\"),\n",
    "                            \"metric_placement\": metric_metadata.get(\"placement\"),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    return flat_records\n",
    "\n",
    "\n",
    "def store_in_parquet(data, parquet_dir):\n",
    "    \"\"\"Store the flattened data in Parquet format\"\"\"\n",
    "    flat_data = flatten_hierarchical_data(data)\n",
    "    df = pd.DataFrame(flat_data)\n",
    "\n",
    "    # Clean parquet_dir\n",
    "    if os.path.exists(parquet_dir):\n",
    "        shutil.rmtree(parquet_dir)\n",
    "\n",
    "    # Create table from DataFrame\n",
    "    table = pa.Table.from_pandas(df)\n",
    "\n",
    "    # Write to Parquet file with partitioning\n",
    "    start_time = time.time()\n",
    "    pq.write_to_dataset(\n",
    "        table,\n",
    "        root_path=parquet_dir,\n",
    "        partition_cols=[\"run_id\", \"module_id\"],  # Partition by run_id and module_id\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "def query_single_item_mongodb(uri=MONGO_URI, db_name=MONGO_DB_NAME, collection_name=MONGO_COLLECTION_NAME):\n",
    "    \"\"\"Query MongoDB to retrieve specific metric values\"\"\"\n",
    "\n",
    "    client = pymongo.MongoClient(uri)\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    # Constructing the MongoDB query to find runs matching criteria\n",
    "    query = {\"run_id\": \"run_0\"}\n",
    "    projection = {\"run_id\": 1, \"modules\": {\"$elemMatch\": {\"samples.metrics\": 1}}}\n",
    "\n",
    "    start_time = time.time()\n",
    "    results = list(collection.find(query, projection))\n",
    "    end_time = time.time()\n",
    "\n",
    "    return results, end_time - start_time\n",
    "\n",
    "\n",
    "def query_single_metric_mongodb(uri=MONGO_URI, db_name=MONGO_DB_NAME, collection_name=MONGO_COLLECTION_NAME):\n",
    "    \"\"\"Query MongoDB to retrieve specific metric values\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    client = pymongo.MongoClient(uri)\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    # Example query: Get all values for metric_0 in module_0 for runs with specific criteria\n",
    "    metric_name = \"metric_0\"\n",
    "\n",
    "    # Using MongoDB's aggregation pipeline to unwind arrays and extract specific metric values\n",
    "    pipeline = [\n",
    "        # Unwind modules array\n",
    "        {\"$unwind\": \"$modules\"},\n",
    "        # Unwind samples array\n",
    "        {\"$unwind\": \"$modules.samples\"},\n",
    "        # Create a field for each metric and its value using $objectToArray to convert metrics to array\n",
    "        {\"$addFields\": {\n",
    "            \"metricsArray\": {\"$objectToArray\": \"$modules.samples.metrics\"}\n",
    "        }},\n",
    "        # Unwind metrics array\n",
    "        {\"$unwind\": \"$metricsArray\"},\n",
    "        # Filter to keep only the specific metric we want\n",
    "        {\"$match\": {\"metricsArray.k\": metric_name}},\n",
    "        # Project the fields we need\n",
    "        {\"$project\": {\n",
    "            \"run_id\": 1,\n",
    "            \"sample_id\": \"$modules.samples.sample_id\",\n",
    "            \"metric_name\": \"$metricsArray.k\",\n",
    "            \"metric_value\": \"$metricsArray.v.value\"\n",
    "        }}\n",
    "    ]  \n",
    "    start_time = time.time()    \n",
    "    results = list(collection.aggregate(pipeline))\n",
    "    end_time = time.time()\n",
    "\n",
    "    return results, end_time - start_time\n",
    "\n",
    "\n",
    "def query_single_metric_parquet(parquet_dir=PARQUET_DIR):\n",
    "    \"\"\"Query Parquet files to retrieve specific metric values using DuckDB\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    metric_name = \"metric_0\"\n",
    "\n",
    "    # Use DuckDB to query the Parquet files directly\n",
    "    con = duckdb.connect(database=':memory:')\n",
    "    query = f\"\"\"\n",
    "        SELECT * FROM '{parquet_dir}/**/*.parquet'\n",
    "        WHERE metric_name = '{metric_name}'\n",
    "    \"\"\"\n",
    "    filtered_df = con.execute(query).fetchdf()\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    return filtered_df, end_time - start_time\n",
    "\n",
    "\n",
    "def query_single_metric_parquet_pyarrow(parquet_dir=PARQUET_DIR):\n",
    "    start_time = time.time()\n",
    "\n",
    "    metric_name = \"metric_0\"\n",
    "\n",
    "    dataset = ds.dataset(parquet_dir, format=\"parquet\")\n",
    "    # Define filter condition for the metric name\n",
    "    filter_expr = (ds.field(\"metric_name\") == metric_name)\n",
    "    # Read the filtered data\n",
    "    table = dataset.to_table(filter=filter_expr)\n",
    "    # Convert to pandas DataFrame if needed\n",
    "    df = table.to_pandas()\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    return df, end_time - start_time\n",
    "\n",
    "\n",
    "def query_single_item_parquet(parquet_dir=PARQUET_DIR):\n",
    "    \"\"\"Query Parquet files to retrieve specific items using DuckDB\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Filter by run_id and module_id\n",
    "    run_id = \"run_0\"\n",
    "    module_id = \"module_0\"\n",
    "\n",
    "    # Use DuckDB to query the Parquet files directly\n",
    "    con = duckdb.connect(database=':memory:')\n",
    "    query = f\"\"\"\n",
    "        SELECT * FROM '{parquet_dir}/**/*.parquet'\n",
    "        WHERE run_id = '{run_id}' AND module_id = '{module_id}'\n",
    "    \"\"\"\n",
    "    filtered_df = con.execute(query).fetchdf()\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    return filtered_df, end_time - start_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mongo_size():\n",
    "    client = pymongo.MongoClient(MONGO_URI)\n",
    "    db = client[MONGO_DB_NAME]\n",
    "    stats = db.command(\"collstats\", MONGO_COLLECTION_NAME)\n",
    "    size_bytes = stats['storageSize']\n",
    "    return size_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dir_size(source=PARQUET_DIR):\n",
    "    total_size = os.path.getsize(source)\n",
    "    for item in os.listdir(source):\n",
    "        itempath = os.path.join(source, item)\n",
    "        if os.path.isfile(itempath):\n",
    "            total_size += os.path.getsize(itempath)\n",
    "        elif os.path.isdir(itempath):\n",
    "            total_size += dir_size(itempath)\n",
    "    return total_size\n",
    "\n",
    "dir_size(PARQUET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sample data...\n",
      "Generated 1000 runs with 10 modules each\n",
      "Each module has 100 samples with 20 metrics\n",
      "\n",
      "--- MongoDB Performance ---\n",
      "MongoDB storage time: 21.9993 seconds\n",
      "MongoDB query single metric time: 32.6960 seconds\n",
      "MongoDB results count: 1000000\n",
      "MongoDB query single module time: 0.0100 seconds\n",
      "MongoDB results count: 1\n",
      "\n",
      "--- Parquet Performance ---\n",
      "Parquet storage time: 15.9431 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3e219abc2644f79c71a9d6ec45c0ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet query single metric time (with DuckDB): 9.7788 seconds\n",
      "Parquet results count: 1000000\n",
      "Parquet query single metric time (with pyarrow): 3.4799 seconds\n",
      "Parquet results count: 1000000\n",
      "Parquet query single module time: 0.6225 seconds\n",
      "Parquet results count: 2000\n",
      "\n",
      "--- Size Comparison ---\n",
      "MongoDB storage size: 507.88 MB\n",
      "Parquet storage size: 600.40 MB\n",
      "\n",
      "--- Summary ---\n",
      "MongoDB advantages:\n",
      "- Flexible schema allows for easier updates and schema evolution\n",
      "- Better for nested/hierarchical data without flattening\n",
      "- Powerful query capabilities and indexing\n",
      "- Good for low-latency individual record access\n",
      "\n",
      "Parquet advantages:\n",
      "- Columnar storage optimizes for analytical queries on specific metrics\n",
      "- Better compression ratio (typically 2-4x smaller than JSON)\n",
      "- Efficient for batch processing and analytics\n",
      "- Schema enforcement and type safety\n",
      "- Works well with big data processing tools (Spark, Presto, etc.)\n",
      "\n",
      "Recommendation:\n",
      "For your use case, MongoDB might be more efficient for queries.\n",
      "\n",
      "Considerations:\n",
      "1. If you need to query across many runs and metrics, Parquet's columnar nature should be better\n",
      "2. If you primarily access complete records for specific runs, MongoDB should be better\n",
      "3. For very large datasets (millions of runs), consider a hybrid approach:\n",
      "   - Use MongoDB for recent/hot data with frequent access patterns\n",
      "   - Archive older data to Parquet for long-term storage and batch analytics\n",
      "4. Partitioning strategies are critical for both approaches as data grows\n",
      "5. Consider using MongoDB for OLTP workloads and Parquet for OLAP workloads\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating sample data...\")\n",
    "data = generate_all_data(\n",
    "    NUM_RUNS, NUM_MODULES, NUM_SAMPLES_PER_MODULE, NUM_METRICS_PER_MODULE\n",
    ")\n",
    "print(f\"Generated {NUM_RUNS} runs with {NUM_MODULES} modules each\")\n",
    "print(\n",
    "    f\"Each module has {NUM_SAMPLES_PER_MODULE} samples with {NUM_METRICS_PER_MODULE} metrics\"\n",
    ")\n",
    "\n",
    "print(\"\\n--- MongoDB Performance ---\")\n",
    "mongo_store_time = store_in_mongodb(data)\n",
    "print(f\"MongoDB storage time: {mongo_store_time:.4f} seconds\")\n",
    "\n",
    "mongo_results, mongo_query_time = query_single_metric_mongodb()\n",
    "print(f\"MongoDB query single metric time: {mongo_query_time:.4f} seconds\")\n",
    "print(f\"MongoDB results count: {len(mongo_results)}\")\n",
    "mongo_results, mongo_query_time = query_single_item_mongodb()\n",
    "print(f\"MongoDB query single module time: {mongo_query_time:.4f} seconds\")\n",
    "print(f\"MongoDB results count: {len(mongo_results)}\")\n",
    "\n",
    "# Parquet comparison\n",
    "print(\"\\n--- Parquet Performance ---\")\n",
    "parquet_store_time = store_in_parquet(data, PARQUET_DIR)\n",
    "print(f\"Parquet storage time: {parquet_store_time:.4f} seconds\")\n",
    "\n",
    "# Similar query in Parquet\n",
    "parquet_results, parquet_query_time = query_single_metric_parquet()\n",
    "print(f\"Parquet query single metric time (with DuckDB): {parquet_query_time:.4f} seconds\")\n",
    "print(f\"Parquet results count: {len(parquet_results)}\")\n",
    "parquet_results, parquet_query_time = query_single_metric_parquet_pyarrow()\n",
    "print(f\"Parquet query single metric time (with pyarrow): {parquet_query_time:.4f} seconds\")\n",
    "print(f\"Parquet results count: {len(parquet_results)}\")\n",
    "parquet_results, parquet_query_time = query_single_item_parquet()\n",
    "print(f\"Parquet query single module time: {parquet_query_time:.4f} seconds\")\n",
    "print(f\"Parquet results count: {len(parquet_results)}\")\n",
    "\n",
    "print(\"\\n--- Size Comparison ---\")\n",
    "print(f\"MongoDB storage size: {mongo_size() / (1024 * 1024):.2f} MB\")\n",
    "print(f\"Parquet storage size: {dir_size(PARQUET_DIR) / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "print(\"\\n--- Summary ---\")\n",
    "print(\"MongoDB advantages:\")\n",
    "print(\"- Flexible schema allows for easier updates and schema evolution\")\n",
    "print(\"- Better for nested/hierarchical data without flattening\")\n",
    "print(\"- Powerful query capabilities and indexing\")\n",
    "print(\"- Good for low-latency individual record access\")\n",
    "\n",
    "print(\"\\nParquet advantages:\")\n",
    "print(\"- Columnar storage optimizes for analytical queries on specific metrics\")\n",
    "print(\"- Better compression ratio (typically 2-4x smaller than JSON)\")\n",
    "print(\"- Efficient for batch processing and analytics\")\n",
    "print(\"- Schema enforcement and type safety\")\n",
    "print(\"- Works well with big data processing tools (Spark, Presto, etc.)\")\n",
    "\n",
    "print(\"\\nRecommendation:\")\n",
    "if mongo_query_time < parquet_query_time:\n",
    "    print(\"For your use case, MongoDB might be more efficient for queries.\")\n",
    "else:\n",
    "    print(\"For your use case, Parquet might be more efficient for queries.\")\n",
    "\n",
    "print(\"\\nConsiderations:\")\n",
    "print(\n",
    "    \"1. If you need to query across many runs and metrics, Parquet's columnar nature should be better\"\n",
    ")\n",
    "print(\n",
    "    \"2. If you primarily access complete records for specific runs, MongoDB should be better\"\n",
    ")\n",
    "print(\"3. For very large datasets (millions of runs), consider a hybrid approach:\")\n",
    "print(\"   - Use MongoDB for recent/hot data with frequent access patterns\")\n",
    "print(\n",
    "    \"   - Archive older data to Parquet for long-term storage and batch analytics\"\n",
    ")\n",
    "print(\"4. Partitioning strategies are critical for both approaches as data grows\")\n",
    "print(\"5. Consider using MongoDB for OLTP workloads and Parquet for OLAP workloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<clickhouse_connect.driver.query.QueryResult at 0x5980cc9e0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clickhouse_connect\n",
    "import json\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    client = clickhouse_connect.get_client(\n",
    "        host='v8u27izgoz.eu-central-1.aws.clickhouse.cloud',\n",
    "        user='default',\n",
    "        password='RxvHiATGn60~a',\n",
    "        secure=True\n",
    "    )\n",
    "    print(\"Result:\", client.query(\"SELECT 1\").result_set[0][0])\n",
    "\n",
    "CLICKHOUSE_DATABASE = 'data_comparison'\n",
    "\n",
    "# Create database if it doesn't exist\n",
    "client.query(f'CREATE DATABASE IF NOT EXISTS {CLICKHOUSE_DATABASE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<clickhouse_connect.driver.query.QueryResult at 0x5a74429c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.query(f'DROP DATABASE {CLICKHOUSE_DATABASE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<clickhouse_connect.driver.query.QueryResult at 0x5a343d340>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create runs table with semi-structured data\n",
    "client.query(f'''\n",
    "CREATE TABLE IF NOT EXISTS {CLICKHOUSE_DATABASE}.runs (\n",
    "    run_id String,\n",
    "    timestamp DateTime,\n",
    "    modules Nested(\n",
    "        module_id String,\n",
    "        name String,\n",
    "        url String,\n",
    "        comment String,\n",
    "        metrics_metadata String  -- JSON string\n",
    "    ),\n",
    "    samples Nested(\n",
    "        sample_id String,\n",
    "        module_id String,\n",
    "        metrics String  -- JSON string\n",
    "    )\n",
    ") ENGINE = MergeTree()\n",
    "ORDER BY (run_id)\n",
    "''')\n",
    "\n",
    "# Create a materialized view for faster metric queries\n",
    "client.query(f'''\n",
    "CREATE MATERIALIZED VIEW IF NOT EXISTS {CLICKHOUSE_DATABASE}.metric_values\n",
    "ENGINE = MergeTree()\n",
    "ORDER BY (metric_name, run_id, module_id, sample_id)\n",
    "AS WITH \n",
    "    extracted AS (\n",
    "        SELECT\n",
    "            run_id,\n",
    "            modules.module_id AS module_id,\n",
    "            samples.sample_id AS sample_id,\n",
    "            tuple.1 AS metric_name,\n",
    "            tuple.2 AS metric_value\n",
    "        FROM {CLICKHOUSE_DATABASE}.runs\n",
    "        ARRAY JOIN \n",
    "            modules,\n",
    "            samples,\n",
    "            JSONExtractKeysAndValues(samples.metrics, 'String', 'Float64') AS tuple\n",
    "        WHERE samples.module_id = modules.module_id\n",
    "    )\n",
    "SELECT \n",
    "    run_id,\n",
    "    module_id,\n",
    "    sample_id,\n",
    "    metric_name,\n",
    "    metric_value\n",
    "FROM extracted\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "not all arguments converted during string formatting",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Insert data\u001b[39;00m\n\u001b[1;32m     47\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 48\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mINSERT INTO \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mCLICKHOUSE_DATABASE\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.runs VALUES\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrows\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     53\u001b[0m store_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/git/MultiQC/venv312/lib/python3.12/site-packages/clickhouse_connect/driver/client.py:225\u001b[0m, in \u001b[0;36mClient.query\u001b[0;34m(self, query, parameters, settings, query_formats, column_formats, encoding, use_none, column_oriented, use_numpy, max_str_len, context, query_tz, column_tzs, external_data)\u001b[0m\n\u001b[1;32m    223\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlocals\u001b[39m()\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 225\u001b[0m query_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_query_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m query_context\u001b[38;5;241m.\u001b[39mis_command:\n\u001b[1;32m    227\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand(query,\n\u001b[1;32m    228\u001b[0m                             parameters\u001b[38;5;241m=\u001b[39mquery_context\u001b[38;5;241m.\u001b[39mparameters,\n\u001b[1;32m    229\u001b[0m                             settings\u001b[38;5;241m=\u001b[39mquery_context\u001b[38;5;241m.\u001b[39msettings,\n\u001b[1;32m    230\u001b[0m                             external_data\u001b[38;5;241m=\u001b[39mquery_context\u001b[38;5;241m.\u001b[39mexternal_data)\n",
      "File \u001b[0;32m~/git/MultiQC/venv312/lib/python3.12/site-packages/clickhouse_connect/driver/client.py:495\u001b[0m, in \u001b[0;36mClient.create_query_context\u001b[0;34m(self, query, parameters, settings, query_formats, column_formats, encoding, use_none, column_oriented, use_numpy, max_str_len, context, query_tz, column_tzs, use_na_values, streaming, as_pandas, external_data, use_extended_dtypes)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_pandas \u001b[38;5;129;01mand\u001b[39;00m use_extended_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    494\u001b[0m     use_extended_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQueryContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mquery_formats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_formats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcolumn_formats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_formats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mserver_tz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserver_tz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m                    \u001b[49m\u001b[43muse_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_none\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcolumn_oriented\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_oriented\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m                    \u001b[49m\u001b[43muse_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_numpy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmax_str_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_str_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mquery_tz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_tz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcolumn_tzs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_tzs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m                    \u001b[49m\u001b[43muse_extended_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_extended_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mas_pandas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_pandas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mapply_server_tz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_server_timezone\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mexternal_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexternal_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/MultiQC/venv312/lib/python3.12/site-packages/clickhouse_connect/driver/query.py:119\u001b[0m, in \u001b[0;36mQueryContext.__init__\u001b[0;34m(self, query, parameters, settings, query_formats, column_formats, encoding, server_tz, use_none, column_oriented, use_numpy, max_str_len, query_tz, column_tzs, use_extended_dtypes, as_pandas, streaming, apply_server_tz, external_data)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_pandas_na \u001b[38;5;241m=\u001b[39m as_pandas \u001b[38;5;129;01mand\u001b[39;00m pd_extended_dtypes\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming \u001b[38;5;241m=\u001b[39m streaming\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/MultiQC/venv312/lib/python3.12/site-packages/clickhouse_connect/driver/query.py:216\u001b[0m, in \u001b[0;36mQueryContext._update_query\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_query\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_query, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbind_params \u001b[38;5;241m=\u001b[39m \u001b[43mbind_query\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserver_tz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_query, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;66;03m# If we've embedded binary data in the query, all bets are off, and we check the original query for comments\u001b[39;00m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muncommented_query \u001b[38;5;241m=\u001b[39m remove_sql_comments(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery)\n",
      "File \u001b[0;32m~/git/MultiQC/venv312/lib/python3.12/site-packages/clickhouse_connect/driver/binding.py:81\u001b[0m, in \u001b[0;36mbind_query\u001b[0;34m(query, parameters, server_tz)\u001b[0m\n\u001b[1;32m     79\u001b[0m         bound_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparam_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m: format_bind_value(v, server_tz) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m final_params\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     query, bound_params \u001b[38;5;241m=\u001b[39m \u001b[43mfinalize_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_tz\u001b[49m\u001b[43m)\u001b[49m, {}\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m binary_binds:\n\u001b[1;32m     83\u001b[0m     binary_query \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mencode()\n",
      "File \u001b[0;32m~/git/MultiQC/venv312/lib/python3.12/site-packages/clickhouse_connect/driver/binding.py:48\u001b[0m, in \u001b[0;36mfinalize_query\u001b[0;34m(query, parameters, server_tz)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(parameters, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m query \u001b[38;5;241m%\u001b[39m {k: format_query_value(v, server_tz) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m parameters\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquery\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mformat_query_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_tz\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: not all arguments converted during string formatting"
     ]
    }
   ],
   "source": [
    "\"\"\"Store hierarchical data in ClickHouse\"\"\"\n",
    "\n",
    "# Prepare data for insertion\n",
    "rows = []\n",
    "for run in data:\n",
    "    run_id = run[\"run_id\"]\n",
    "    timestamp = run[\"timestamp\"]\n",
    "    \n",
    "    modules_data = {\n",
    "        'module_id': [],\n",
    "        'name': [],\n",
    "        'url': [],\n",
    "        'comment': [],\n",
    "        'metrics_metadata': []\n",
    "    }\n",
    "    \n",
    "    samples_data = {\n",
    "        'sample_id': [],\n",
    "        'module_id': [],\n",
    "        'metrics': []\n",
    "    }\n",
    "    \n",
    "    for module in run[\"modules\"]:\n",
    "        module_id = module[\"module_id\"]\n",
    "        \n",
    "        # Add module info\n",
    "        modules_data['module_id'].append(module_id)\n",
    "        modules_data['name'].append(module[\"name\"])\n",
    "        modules_data['url'].append(module[\"url\"])\n",
    "        modules_data['comment'].append(module[\"comment\"])\n",
    "        modules_data['metrics_metadata'].append(json.dumps(module[\"metrics_metadata\"]))\n",
    "        \n",
    "        # Add samples info\n",
    "        for sample in module[\"samples\"]:\n",
    "            samples_data['sample_id'].append(sample[\"sample_id\"])\n",
    "            samples_data['module_id'].append(module_id)\n",
    "            samples_data['metrics'].append(json.dumps(sample[\"metrics\"]))\n",
    "    \n",
    "    rows.append({\n",
    "        'run_id': run_id,\n",
    "        'timestamp': timestamp,\n",
    "        'modules': modules_data,\n",
    "        'samples': samples_data\n",
    "    })\n",
    "\n",
    "# Insert data\n",
    "start_time = time.time()\n",
    "client.query(\n",
    "    f'INSERT INTO {CLICKHOUSE_DATABASE}.runs VALUES',\n",
    "    rows\n",
    ")\n",
    "end_time = time.time()\n",
    "store_time = end_time - start_time\n",
    "print(f\"ClickHouse storage time: {store_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Query a specific metric across all runs and samples\"\"\"\n",
    "metric_name = \"metric_0\"\n",
    "start_time = time.time()\n",
    "# Query using the materialized view\n",
    "results = client.execute(f'''\n",
    "SELECT run_id, module_id, sample_id, metric_value\n",
    "FROM {CLICKHOUSE_DATABASE}.metric_values\n",
    "WHERE metric_name = %s\n",
    "''', (metric_name,))\n",
    "end_time = time.time()\n",
    "query_time = end_time - start_time\n",
    "print(f\"ClickHouse query time: {query_time:.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
