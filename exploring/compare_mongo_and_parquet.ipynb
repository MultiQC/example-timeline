{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulating MultiQC runs with exporting of data into two types of intermediate format:\n",
    "- columnar data lake (parquet)\n",
    "- document DB (MongoDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import shutil\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pymongo\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "# Configuration parameters\n",
    "NUM_RUNS = 1000  # Can be scaled up to millions in real case\n",
    "NUM_MODULES = 10  # Fixed across runs\n",
    "NUM_SAMPLES_PER_MODULE = 100  # Can be 10 to 1000\n",
    "NUM_METRICS_PER_MODULE = 20  # Can be 10 to 50\n",
    "\n",
    "# MongoDB setup\n",
    "MONGO_DB_NAME = \"data_comparison\"\n",
    "MONGO_COLLECTION_NAME = \"runs\"\n",
    "MONGO_URI = \"mongodb://localhost:27017/\"\n",
    "\n",
    "# Parquet setup\n",
    "PARQUET_DIR = \"parquet_data\"\n",
    "os.makedirs(PARQUET_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def generate_random_string(length=10):\n",
    "    \"\"\"Generate a random string of fixed length\"\"\"\n",
    "    return \"\".join(random.choices(string.ascii_letters, k=length))\n",
    "\n",
    "\n",
    "def generate_metric_metadata():\n",
    "    \"\"\"Generate metadata for a metric\"\"\"\n",
    "    return {\n",
    "        \"min\": random.uniform(0, 10),\n",
    "        \"max\": random.uniform(90, 100),\n",
    "        \"scale\": random.choice([\"linear\", \"log\"]),\n",
    "        \"color\": f\"#{random.randint(0, 0xFFFFFF):06x}\",\n",
    "        \"type\": random.choice([\"numeric\", \"categorical\", \"percentage\"]),\n",
    "        \"namespace\": random.choice([\"performance\", \"quality\", \"resource\"]),\n",
    "        \"placement\": random.choice([\"primary\", \"secondary\", \"tertiary\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_value_metadata(value):\n",
    "    \"\"\"Generate metadata for a value\"\"\"\n",
    "    return {\n",
    "        \"unmodified_value\": value,\n",
    "        \"formatted_value\": f\"{value:.2f}\" if isinstance(value, float) else str(value),\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_sample_data(num_metrics):\n",
    "    \"\"\"Generate data for a single sample\"\"\"\n",
    "    sample_id = generate_random_string()\n",
    "    metrics = {}\n",
    "\n",
    "    for i in range(num_metrics):\n",
    "        metric_name = f\"metric_{i}\"\n",
    "        value = random.uniform(0, 100)\n",
    "        metrics[metric_name] = {\n",
    "            \"value\": value,\n",
    "            \"metadata\": generate_value_metadata(value),\n",
    "        }\n",
    "\n",
    "    return {\"sample_id\": sample_id, \"metrics\": metrics}\n",
    "\n",
    "\n",
    "def generate_module_data(module_index, num_samples, num_metrics):\n",
    "    \"\"\"Generate data for a single module\"\"\"\n",
    "    samples = [generate_sample_data(num_metrics) for _ in range(num_samples)]\n",
    "\n",
    "    metrics_metadata = {}\n",
    "    for i in range(num_metrics):\n",
    "        metric_name = f\"metric_{i}\"\n",
    "        metrics_metadata[metric_name] = generate_metric_metadata()\n",
    "\n",
    "    return {\n",
    "        \"module_id\": f\"module_{module_index}\",\n",
    "        \"name\": f\"Module {module_index}\",\n",
    "        \"url\": f\"http://example.com/module/{module_index}\",\n",
    "        \"comment\": f\"This is module {module_index}\",\n",
    "        \"metrics_metadata\": metrics_metadata,\n",
    "        \"samples\": samples,\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_run_data(\n",
    "    run_index, num_modules, num_samples_per_module, num_metrics_per_module\n",
    "):\n",
    "    \"\"\"Generate data for a single run\"\"\"\n",
    "    modules = [\n",
    "        generate_module_data(i, num_samples_per_module, num_metrics_per_module)\n",
    "        for i in range(num_modules)\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"run_id\": f\"run_{run_index}\",\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"modules\": modules,\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_all_data(\n",
    "    num_runs, num_modules, num_samples_per_module, num_metrics_per_module\n",
    "):\n",
    "    \"\"\"Generate all runs data\"\"\"\n",
    "    return [\n",
    "        generate_run_data(\n",
    "            i, num_modules, num_samples_per_module, num_metrics_per_module\n",
    "        )\n",
    "        for i in range(num_runs)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_in_mongodb(data, uri=MONGO_URI, db_name=MONGO_DB_NAME, collection_name=MONGO_COLLECTION_NAME):\n",
    "    \"\"\"Store the hierarchical data in MongoDB\"\"\"\n",
    "    client = pymongo.MongoClient(uri)\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    # Drop the collection if it already exists\n",
    "    collection.drop()\n",
    "\n",
    "    # Insert the data\n",
    "    start_time = time.time()\n",
    "    collection.insert_many(data)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Create indexes for common queries\n",
    "    collection.create_index([(\"run_id\", pymongo.ASCENDING)])\n",
    "    collection.create_index([(\"modules.module_id\", pymongo.ASCENDING)])\n",
    "    collection.create_index([(\"modules.samples.sample_id\", pymongo.ASCENDING)])\n",
    "\n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "def flatten_hierarchical_data(data):\n",
    "    \"\"\"Convert hierarchical data to flat format for Parquet\"\"\"\n",
    "    flat_records = []\n",
    "\n",
    "    for run in data:\n",
    "        run_id = run[\"run_id\"]\n",
    "        timestamp = run[\"timestamp\"]\n",
    "\n",
    "        for module in run[\"modules\"]:\n",
    "            module_id = module[\"module_id\"]\n",
    "            module_name = module[\"name\"]\n",
    "            module_url = module[\"url\"]\n",
    "            module_comment = module[\"comment\"]\n",
    "\n",
    "            for sample in module[\"samples\"]:\n",
    "                sample_id = sample[\"sample_id\"]\n",
    "\n",
    "                for metric_name, metric_data in sample[\"metrics\"].items():\n",
    "                    value = metric_data[\"value\"]\n",
    "                    unmodified_value = metric_data[\"metadata\"][\"unmodified_value\"]\n",
    "                    formatted_value = metric_data[\"metadata\"][\"formatted_value\"]\n",
    "\n",
    "                    # Get metric metadata\n",
    "                    metric_metadata = module[\"metrics_metadata\"].get(metric_name, {})\n",
    "\n",
    "                    flat_records.append(\n",
    "                        {\n",
    "                            \"run_id\": run_id,\n",
    "                            \"timestamp\": timestamp,\n",
    "                            \"module_id\": module_id,\n",
    "                            \"module_name\": module_name,\n",
    "                            \"module_url\": module_url,\n",
    "                            \"module_comment\": module_comment,\n",
    "                            \"sample_id\": sample_id,\n",
    "                            \"metric_name\": metric_name,\n",
    "                            \"value\": value,\n",
    "                            \"unmodified_value\": unmodified_value,\n",
    "                            \"formatted_value\": formatted_value,\n",
    "                            \"metric_min\": metric_metadata.get(\"min\"),\n",
    "                            \"metric_max\": metric_metadata.get(\"max\"),\n",
    "                            \"metric_scale\": metric_metadata.get(\"scale\"),\n",
    "                            \"metric_color\": metric_metadata.get(\"color\"),\n",
    "                            \"metric_type\": metric_metadata.get(\"type\"),\n",
    "                            \"metric_namespace\": metric_metadata.get(\"namespace\"),\n",
    "                            \"metric_placement\": metric_metadata.get(\"placement\"),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    return flat_records\n",
    "\n",
    "\n",
    "def store_in_parquet(data, parquet_dir):\n",
    "    \"\"\"Store the flattened data in Parquet format\"\"\"\n",
    "    flat_data = flatten_hierarchical_data(data)\n",
    "    df = pd.DataFrame(flat_data)\n",
    "\n",
    "    # Clean parquet_dir\n",
    "    if os.path.exists(parquet_dir):\n",
    "        shutil.rmtree(parquet_dir)\n",
    "\n",
    "    # Create table from DataFrame\n",
    "    table = pa.Table.from_pandas(df)\n",
    "\n",
    "    # Write to Parquet file with partitioning\n",
    "    start_time = time.time()\n",
    "    pq.write_to_dataset(\n",
    "        table,\n",
    "        root_path=parquet_dir,\n",
    "        partition_cols=[\"run_id\", \"module_id\"],  # Partition by run_id and module_id\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "def query_single_item_mongodb(uri=MONGO_URI, db_name=MONGO_DB_NAME, collection_name=MONGO_COLLECTION_NAME):\n",
    "    \"\"\"Query MongoDB to retrieve specific metric values\"\"\"\n",
    "\n",
    "    client = pymongo.MongoClient(uri)\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    # Constructing the MongoDB query to find runs matching criteria\n",
    "    query = {\"run_id\": \"run_0\"}\n",
    "    projection = {\"run_id\": 1, \"modules\": {\"$elemMatch\": {\"samples.metrics\": 1}}}\n",
    "\n",
    "    start_time = time.time()\n",
    "    results = list(collection.find(query, projection))\n",
    "    end_time = time.time()\n",
    "\n",
    "    return results, end_time - start_time\n",
    "\n",
    "\n",
    "def query_single_metric_mongodb(uri=MONGO_URI, db_name=MONGO_DB_NAME, collection_name=MONGO_COLLECTION_NAME):\n",
    "    \"\"\"Query MongoDB to retrieve specific metric values\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    client = pymongo.MongoClient(uri)\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    # Example query: Get all values for metric_0 in module_0 for runs with specific criteria\n",
    "    metric_name = \"metric_0\"\n",
    "\n",
    "    # Using MongoDB's aggregation pipeline to unwind arrays and extract specific metric values\n",
    "    pipeline = [\n",
    "        # Unwind modules array\n",
    "        {\"$unwind\": \"$modules\"},\n",
    "        # Unwind samples array\n",
    "        {\"$unwind\": \"$modules.samples\"},\n",
    "        # Create a field for each metric and its value using $objectToArray to convert metrics to array\n",
    "        {\"$addFields\": {\n",
    "            \"metricsArray\": {\"$objectToArray\": \"$modules.samples.metrics\"}\n",
    "        }},\n",
    "        # Unwind metrics array\n",
    "        {\"$unwind\": \"$metricsArray\"},\n",
    "        # Filter to keep only the specific metric we want\n",
    "        {\"$match\": {\"metricsArray.k\": metric_name}},\n",
    "        # Project the fields we need\n",
    "        {\"$project\": {\n",
    "            \"run_id\": 1,\n",
    "            \"sample_id\": \"$modules.samples.sample_id\",\n",
    "            \"metric_name\": \"$metricsArray.k\",\n",
    "            \"metric_value\": \"$metricsArray.v.value\"\n",
    "        }}\n",
    "    ]  \n",
    "    start_time = time.time()    \n",
    "    results = list(collection.aggregate(pipeline))\n",
    "    end_time = time.time()\n",
    "\n",
    "    return results, end_time - start_time\n",
    "\n",
    "\n",
    "def query_single_metric_parquet(parquet_dir=PARQUET_DIR):\n",
    "    \"\"\"Query Parquet files to retrieve specific metric values using DuckDB\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    metric_name = \"metric_0\"\n",
    "\n",
    "    # Use DuckDB to query the Parquet files directly\n",
    "    con = duckdb.connect(database=':memory:')\n",
    "    query = f\"\"\"\n",
    "        SELECT * FROM '{parquet_dir}/**/*.parquet'\n",
    "        WHERE metric_name = '{metric_name}'\n",
    "    \"\"\"\n",
    "    filtered_df = con.execute(query).fetchdf()\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    return filtered_df, end_time - start_time\n",
    "\n",
    "\n",
    "def query_single_metric_parquet_pyarrow(parquet_dir=PARQUET_DIR):\n",
    "    start_time = time.time()\n",
    "\n",
    "    metric_name = \"metric_0\"\n",
    "\n",
    "    dataset = ds.dataset(parquet_dir, format=\"parquet\")\n",
    "    # Define filter condition for the metric name\n",
    "    filter_expr = (ds.field(\"metric_name\") == metric_name)\n",
    "    # Read the filtered data\n",
    "    table = dataset.to_table(filter=filter_expr)\n",
    "    # Convert to pandas DataFrame if needed\n",
    "    df = table.to_pandas()\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    return df, end_time - start_time\n",
    "\n",
    "\n",
    "def query_single_item_parquet(parquet_dir=PARQUET_DIR):\n",
    "    \"\"\"Query Parquet files to retrieve specific items using DuckDB\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Filter by run_id and module_id\n",
    "    run_id = \"run_0\"\n",
    "    module_id = \"module_0\"\n",
    "\n",
    "    # Use DuckDB to query the Parquet files directly\n",
    "    con = duckdb.connect(database=':memory:')\n",
    "    query = f\"\"\"\n",
    "        SELECT * FROM '{parquet_dir}/**/*.parquet'\n",
    "        WHERE run_id = '{run_id}' AND module_id = '{module_id}'\n",
    "    \"\"\"\n",
    "    filtered_df = con.execute(query).fetchdf()\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    return filtered_df, end_time - start_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mongo_size():\n",
    "    client = pymongo.MongoClient(MONGO_URI)\n",
    "    db = client[MONGO_DB_NAME]\n",
    "    stats = db.command(\"collstats\", MONGO_COLLECTION_NAME)\n",
    "    size_bytes = stats['storageSize']\n",
    "    return size_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "629559315"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dir_size(source=PARQUET_DIR):\n",
    "    total_size = os.path.getsize(source)\n",
    "    for item in os.listdir(source):\n",
    "        itempath = os.path.join(source, item)\n",
    "        if os.path.isfile(itempath):\n",
    "            total_size += os.path.getsize(itempath)\n",
    "        elif os.path.isdir(itempath):\n",
    "            total_size += dir_size(itempath)\n",
    "    return total_size\n",
    "\n",
    "dir_size(PARQUET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sample data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating sample data...\")\n",
    "data = generate_all_data(\n",
    "    NUM_RUNS, NUM_MODULES, NUM_SAMPLES_PER_MODULE, NUM_METRICS_PER_MODULE\n",
    ")\n",
    "print(f\"Generated {NUM_RUNS} runs with {NUM_MODULES} modules each\")\n",
    "print(\n",
    "    f\"Each module has {NUM_SAMPLES_PER_MODULE} samples with {NUM_METRICS_PER_MODULE} metrics\"\n",
    ")\n",
    "\n",
    "print(\"\\n--- MongoDB Performance ---\")\n",
    "mongo_store_time = store_in_mongodb(data)\n",
    "print(f\"MongoDB storage time: {mongo_store_time:.4f} seconds\")\n",
    "\n",
    "mongo_results, mongo_query_time = query_single_metric_mongodb()\n",
    "print(f\"MongoDB query single metric time: {mongo_query_time:.4f} seconds\")\n",
    "print(f\"MongoDB results count: {len(mongo_results)}\")\n",
    "mongo_results, mongo_query_time = query_single_item_mongodb()\n",
    "print(f\"MongoDB query single module time: {mongo_query_time:.4f} seconds\")\n",
    "print(f\"MongoDB results count: {len(mongo_results)}\")\n",
    "\n",
    "# Parquet comparison\n",
    "print(\"\\n--- Parquet Performance ---\")\n",
    "parquet_store_time = store_in_parquet(data, PARQUET_DIR)\n",
    "print(f\"Parquet storage time: {parquet_store_time:.4f} seconds\")\n",
    "\n",
    "# Similar query in Parquet\n",
    "parquet_results, parquet_query_time = query_single_metric_parquet()\n",
    "print(f\"Parquet query single metric time (with DuckDB): {parquet_query_time:.4f} seconds\")\n",
    "print(f\"Parquet results count: {len(parquet_results)}\")\n",
    "parquet_results, parquet_query_time = query_single_metric_parquet_pyarrow()\n",
    "print(f\"Parquet query single metric time (with pyarrow): {parquet_query_time:.4f} seconds\")\n",
    "print(f\"Parquet results count: {len(parquet_results)}\")\n",
    "parquet_results, parquet_query_time = query_single_item_parquet()\n",
    "print(f\"Parquet query single module time: {parquet_query_time:.4f} seconds\")\n",
    "print(f\"Parquet results count: {len(parquet_results)}\")\n",
    "\n",
    "print(\"\\n--- Size Comparison ---\")\n",
    "print(f\"MongoDB storage size: {mongo_size() / (1024 * 1024):.2f} MB\")\n",
    "print(f\"Parquet storage size: {dir_size(PARQUET_DIR) / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "print(\"\\n--- Summary ---\")\n",
    "print(\"MongoDB advantages:\")\n",
    "print(\"- Flexible schema allows for easier updates and schema evolution\")\n",
    "print(\"- Better for nested/hierarchical data without flattening\")\n",
    "print(\"- Powerful query capabilities and indexing\")\n",
    "print(\"- Good for low-latency individual record access\")\n",
    "\n",
    "print(\"\\nParquet advantages:\")\n",
    "print(\"- Columnar storage optimizes for analytical queries on specific metrics\")\n",
    "print(\"- Better compression ratio (typically 2-4x smaller than JSON)\")\n",
    "print(\"- Efficient for batch processing and analytics\")\n",
    "print(\"- Schema enforcement and type safety\")\n",
    "print(\"- Works well with big data processing tools (Spark, Presto, etc.)\")\n",
    "\n",
    "print(\"\\nRecommendation:\")\n",
    "if mongo_query_time < parquet_query_time:\n",
    "    print(\"For your use case, MongoDB might be more efficient for queries.\")\n",
    "else:\n",
    "    print(\"For your use case, Parquet might be more efficient for queries.\")\n",
    "\n",
    "print(\"\\nConsiderations:\")\n",
    "print(\n",
    "    \"1. If you need to query across many runs and metrics, Parquet's columnar nature should be better\"\n",
    ")\n",
    "print(\n",
    "    \"2. If you primarily access complete records for specific runs, MongoDB should be better\"\n",
    ")\n",
    "print(\"3. For very large datasets (millions of runs), consider a hybrid approach:\")\n",
    "print(\"   - Use MongoDB for recent/hot data with frequent access patterns\")\n",
    "print(\n",
    "    \"   - Archive older data to Parquet for long-term storage and batch analytics\"\n",
    ")\n",
    "print(\"4. Partitioning strategies are critical for both approaches as data grows\")\n",
    "print(\"5. Consider using MongoDB for OLTP workloads and Parquet for OLAP workloads\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
